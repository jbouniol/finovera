"""
daily_update.py â€“ Mise Ã  jour automatique des donnÃ©es marchÃ© et news pour Finovera
===================================================================================

Ce module gÃ¨re la rÃ©cupÃ©ration quotidienne des donnÃ©es boursiÃ¨res et des actualitÃ©s financiÃ¨res
pour chaque ticker suivi par lâ€™application. Les donnÃ©es sont enrichies par lâ€™analyse de sentiment
des titres de news. Les rÃ©sultats sont sauvegardÃ©s dans deux fichiersÂ : le dataset final (actions)
et un historique des news scorÃ©es.

Principales Ã©tapesÂ :
- TÃ©lÃ©chargement incrÃ©mental des prix et volumes avec yfinance
- RÃ©cupÃ©ration des news rÃ©centes et scoring sentiment avec Vader
- Fusion, agrÃ©gation et sauvegarde des rÃ©sultats
- Affichage en temps rÃ©el de lâ€™avancement via Streamlit

EntrÃ©esÂ : fichiers CSV existants (donnÃ©es marchÃ©, news)
SortiesÂ : CSV mis Ã  jour (final_dataset.csv, news_data.csv), feedback Streamlit

DerniÃ¨re mise Ã  jourÂ : 2025-05-14
"""

import pandas as pd
import yfinance as yf
from newsapi import NewsApiClient
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import datetime
import os
import traceback
import streamlit as st

# === CONFIGURATION API ET CHEMINS ===
NEWS_API_KEY = "e67a21b3ecc14ee395ea4256670b8af7"
newsapi = NewsApiClient(api_key=NEWS_API_KEY)
analyzer = SentimentIntensityAnalyzer()

DATA_PATH = "data/"
FINAL_DATASET = DATA_PATH + "final_dataset.csv"
NEWS_DATA = DATA_PATH + "news_data.csv"

def daily_update():
    """
    Met Ã  jour le dataset actions et le fichier news_data.csv pour tous les tickers suivis.
    RÃ©cupÃ¨re les nouvelles donnÃ©es boursiÃ¨res manquantes et les derniÃ¨res news pour chaque ticker.
    Analyse le sentiment des titres de news et fusionne le tout dans les fichiers finaux.
    Affiche lâ€™avancement et les logs via Streamlit.

    - RÃ©cupÃ¨re les nouveaux jours de marchÃ© manquants pour chaque ticker via yfinance
    - Pour chaque ticker, agrÃ¨ge les derniÃ¨res news sur 7 jours glissants, calcule un score sentiment moyen par jour
    - Fusionne et sauvegarde les rÃ©sultats dans les CSV du projet
    - Affiche lâ€™avancement dans lâ€™UI
    """
    try:
        df_final = pd.read_csv(FINAL_DATASET)
        df_final["Date"] = pd.to_datetime(df_final["Date"])
    except FileNotFoundError:
        st.error("âŒ final_dataset.csv introuvable.")
        return

    all_tickers = df_final["Ticker"].unique()
    st.info(f"ðŸ”Ž {len(all_tickers)} tickers trouvÃ©s dans la base Ã  mettre Ã  jour : {list(all_tickers)}")

    try:
        df_news = pd.read_csv(NEWS_DATA)
        df_news["date"] = pd.to_datetime(df_news["publishedAt"]).dt.date
        df_news["date"] = pd.to_datetime(df_news["date"])
    except FileNotFoundError:
        df_news = pd.DataFrame()

    all_updates = []
    new_news = []

    today = datetime.date.today()
    last_week = today - datetime.timedelta(days=7)

    with st.spinner("Mise Ã  jour en cours..."):
        progress = st.progress(0)
        for i, ticker in enumerate(all_tickers):
            try:
                st.write(f"\nðŸ”„ Mise Ã  jour de {ticker}...")

                # 1. Recherche du dernier jour connu pour ce ticker
                last_date = df_final[df_final["Ticker"] == ticker]["Date"].max().date()
                start_date = last_date + datetime.timedelta(days=1)
                if start_date >= today:
                    st.write("âœ… DonnÃ©es dÃ©jÃ  Ã  jour.")
                    progress.progress((i + 1) / len(all_tickers))
                    continue

                # 2. TÃ©lÃ©chargement des donnÃ©es boursiÃ¨res manquantes via yfinance
                df_raw = yf.download(ticker, start=start_date.isoformat(), end=today.isoformat())
                if df_raw.empty:
                    st.warning(f"âŒ Aucune nouvelle donnÃ©e boursiÃ¨re pour {ticker}")
                    progress.progress((i + 1) / len(all_tickers))
                    continue

                df_raw.reset_index(inplace=True)
                if isinstance(df_raw.columns, pd.MultiIndex):
                    # Aplatit MultiIndex si besoin
                    df_raw.columns = [col[0] if col[1] == '' else f"{col[0]}_{col[1]}" for col in df_raw.columns]

                # 3. Construction DataFrame des nouveaux jours
                df_stock = pd.DataFrame({
                    "Date": pd.to_datetime(df_raw["Date"]),
                    "Ticker": ticker,
                    "Open": pd.to_numeric(df_raw.get("Open"), errors="coerce"),
                    "High": pd.to_numeric(df_raw.get("High"), errors="coerce"),
                    "Low": pd.to_numeric(df_raw.get("Low"), errors="coerce"),
                    "Close": pd.to_numeric(df_raw.get("Close"), errors="coerce"),
                    "Volume": pd.to_numeric(df_raw.get("Volume"), errors="coerce"),
                })

                df_stock.dropna(subset=["Close"], inplace=True)
                df_stock.sort_values(by="Date", inplace=True)
                df_stock["next_close"] = df_stock["Close"].shift(-1)
                df_stock["variation_pct"] = (df_stock["next_close"] - df_stock["Close"]) / df_stock["Close"]

                last_downloaded_date = df_stock["Date"].max().date()
                if last_downloaded_date == datetime.date.today():
                    print(f"âœ… DonnÃ©es du jour {last_downloaded_date} bien prÃ©sentes.")
                else:
                    print(f"âš ï¸ DerniÃ¨re donnÃ©e pour {ticker} : {last_downloaded_date} (pas encore aujourdâ€™hui).")

                # 4. RÃ©cupÃ©ration des news des 7 derniers jours
                try:
                    news = newsapi.get_everything(
                        q=ticker,
                        from_param=last_week.isoformat(),
                        to=today.isoformat(),
                        language="en",
                        sort_by="relevancy",
                        page=1,
                        page_size=10,
                    )
                except Exception as e:
                    if "rateLimited" in str(e):
                        st.warning(f"ðŸ›‘ Limite NewsAPI atteinte pour {ticker} â€“ skipping news.")
                        news = {"articles": []}
                    else:
                        raise e

                # 5. Calcul du score de sentiment sur les titres des news
                df_n = pd.DataFrame([{
                    "ticker": ticker,
                    "title": a["title"],
                    "publishedAt": a["publishedAt"],
                    "source": a["source"]["name"],
                    "url": a["url"],
                    "sentiment": analyzer.polarity_scores(a["title"])['compound']
                } for a in news["articles"]])

                if not df_n.empty:
                    df_n["date"] = pd.to_datetime(df_n["publishedAt"]).dt.date
                    df_n["date"] = pd.to_datetime(df_n["date"])
                    new_news.append(df_n)

                    # 6. Moyenne journaliÃ¨re du sentiment pour chaque jour avec news
                    df_sentiment = df_n.groupby("date")["sentiment"].mean().reset_index()
                    df_sentiment.rename(columns={"date": "Date"}, inplace=True)
                    df_sentiment["Ticker"] = ticker

                    # 7. Ã‰tend le score sentiment Ã  tous les jours boursiers, remplit les jours sans news par 0
                    all_dates = df_stock["Date"].unique()
                    df_sentiment = df_sentiment.set_index("Date").reindex(all_dates).fillna(0.0).reset_index()
                    df_sentiment["Ticker"] = ticker

                    # 8. Fusionne prix + sentiment pour chaque jour boursier
                    df_merged = pd.merge(df_stock, df_sentiment, on=["Date", "Ticker"], how="left")

                else:
                    df_merged = df_stock.copy()
                    df_merged["sentiment"] = 0

                # 9. PrÃ©paration du dataframe final pour ce ticker
                df_merged = df_merged[["Date", "Ticker", "sentiment", "variation_pct", "Open", "Close", "High", "Low", "Volume"]]
                df_merged.dropna(subset=["variation_pct"], inplace=True)

                if df_merged.empty:
                    st.warning(f"âš ï¸ Pas de ligne Ã  ajouter pour {ticker}")
                    progress.progress((i + 1) / len(all_tickers))
                    continue

                all_updates.append(df_merged)
                st.success(f"âœ… {len(df_merged)} lignes ajoutÃ©es pour {ticker}")

            except Exception as e:
                st.error(f"âŒ Erreur pour {ticker} : {e}")
                st.text(traceback.format_exc())

            progress.progress((i + 1) / len(all_tickers))

    # 10. Sauvegarde des nouvelles donnÃ©es marchÃ©
    if all_updates:
        df_new = pd.concat(all_updates, ignore_index=True)
        df_full = pd.concat([df_final, df_new], ignore_index=True)
        df_full.drop_duplicates(subset=["Date", "Ticker"], keep="last", inplace=True)

        # Garder uniquement les 30 derniers jours (rolling window)
        rolling_cutoff = df_full["Date"].max() - pd.Timedelta(days=30)
        df_full = df_full[df_full["Date"] >= rolling_cutoff]

        df_full.to_csv(FINAL_DATASET, index=False)
        st.success(f"ðŸ’¾ final_dataset.csv mis Ã  jour avec {len(df_new)} nouvelles lignes")

    # 11. Sauvegarde des news enrichies
    if new_news:
        df_new_news = pd.concat(new_news, ignore_index=True)
        df_all_news = pd.concat([df_news, df_new_news], ignore_index=True)
        df_all_news.to_csv(NEWS_DATA, index=False)
        st.success(f"ðŸ“° news_data.csv mis Ã  jour avec {len(df_new_news)} nouvelles lignes")
